model:
  name: "EleutherAI/pythia-125m"
  tokenizer: "EleutherAI/pythia-125m"
  instruction_token: False

lora:
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules: null
  bias: "none"

train_dataset:
  name: "medpaca"
  files: "/nfs-share/pa511/llm_memorisation/datasets_our/medical_dataset/deduplicated_medical_meadow_flashcards_train.json"
  split: "train"
  text_field: "text"

validation_dataset:
  name: "medpaca"
  files: "/nfs-share/pa511/llm_memorisation/datasets_our/medical_dataset/deduplicated_medical_meadow_flashcards_non_member.json"
  split: "train"
  text_field: "text"

training:
  per_device_train_batch_size: 2
  logging_dir: "./logs"
  logging_steps: 1
  max_seq_length: 2048
  num_train_epochs: 1
  eval_strategy: "epoch"
  eval_on_start: True
  report_to: "none"
  save_strategy: "no"
  dataset_text_field: "text"
  warmup_ratio: 0.1
  batch_eval_metrics: True
  learning_rate: 1e-2

evaluation:
  output_dir: "./results"
  per_device_eval_batch_size: 8

wandb:
  project: "federated-flwr"
  wandb_config:
    model_name: ${model.name}
    tokenizer: ${model.tokenizer}
    instruction_token: ${model.instruction_token}
    lora_r: ${lora.r}
    lora_alpha: ${lora.alpha}
    lora_dropout: ${lora.dropout}
    train_dataset_name: ${train_dataset.name}
    validation_dataset_name: ${validation_dataset.name}
    per_device_train_batch_size: ${training.per_device_train_batch_size}
    max_seq_length: ${training.max_seq_length}
    num_train_epochs: ${training.num_train_epochs}
    evaluation_strategy: ${training.eval_strategy}
    num_clients: ${simulation.num_clients}
    num_rounds: ${simulation.num_rounds}

simulation:
  num_clients: 4
  num_rounds: 60 # allways multiple of eqivalent_cent_epochs
  warmup_ratio: 0.1
  cooloff_ratio: 0.1
  eqivalent_cent_epochs: 20
  client_resources:
    num_cpus: 8
    num_gpus: 1.0