project: "federated-flwr"
model:
  name: "EleutherAI/pythia-125m"
  tokenizer: "EleutherAI/pythia-125m"
  instruction_token: False

lora:
  r: 8
  alpha: 8
  dropout: 0.1
  target_modules: null
  bias: "none"

train_dataset:
  name: "medpaca"
  files: "/nfs-share/pa511/llm_memorisation/datasets_our/medical_dataset/deduplicated_medical_meadow_flashcards_train.json"
  split: "train"
  text_field: "text"

validation_dataset:
  name: "medpaca"
  files: "/nfs-share/pa511/llm_memorisation/datasets_our/medical_dataset/deduplicated_medical_meadow_flashcards_non_member.json"
  split: "train"
  text_field: "text"

training:
  per_device_train_batch_size: 8
  logging_dir: "./logs"
  logging_steps: 10
  max_seq_length: 2048
  max_steps: 75420
  evaluation_strategy: "steps"
  eval_on_start: True
  eval_steps: 1000 # Evaluate every 10 steps
  report_to: "none"
  dataset_text_field: "text"
  batch_eval_metrics: True
  learning_rate: 1e-3
  save_strategy : "best"
  load_best_model_at_end: True
  save_total_limit: 2
  metric_for_best_model: "eval_rouge1"


evaluation:
  output_dir: "./results"
  per_device_eval_batch_size: 8

simulation:
  num_clients: 4
  num_rounds: 50
  warmup_ratio: 0.1
  cooloff_ratio: 0.1
  use_lora: True
  client_resources:
    num_cpus: 8
    num_gpus: 1.0

run_id: "b0jg9cbd3"