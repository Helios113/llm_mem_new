project: "federated-flwr"
model:
  name: "EleutherAI/pythia-125m"
  tokenizer: "EleutherAI/pythia-125m"
  instruction_token: False

lora:
  r: 8
  alpha: 8
  dropout: 0.1
  target_modules: null
  bias: "none"

train_dataset:
  name: "medpaca"
  files: "/nfs-share/pa511/llm_memorisation/datasets_our/medical_dataset/deduplicated_medical_meadow_flashcards_train.json"
  split: "train"
  text_field: "text"

validation_dataset:
  name: "medpaca"
  files: "/nfs-share/pa511/llm_memorisation/datasets_our/medical_dataset/deduplicated_medical_meadow_flashcards_non_member.json"
  split: "train"
  text_field: "text"

training:
  per_device_train_batch_size: 2
  logging_dir: "./logs"
  logging_steps: 10
  max_seq_length: 2048
  num_train_epochs: 1
  eval_strategy: "epoch"
  eval_on_start: True
  report_to: "none"
  save_strategy: "no"
  dataset_text_field: "text"
  warmup_ratio: 0.1
  batch_eval_metrics: True
  learning_rate: 1e-2

evaluation:
  output_dir: "./results"
  per_device_eval_batch_size: 8

simulation:
  num_clients: 4
  num_rounds: 60 # allways multiple of eqivalent_cent_epochs
  warmup_ratio: 0.1
  cooloff_ratio: 0.1
  eqivalent_cent_epochs: 20
  client_resources:
    num_cpus: 8
    num_gpus: 1.0